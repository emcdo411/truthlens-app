[![Made with FastAPI](https://img.shields.io/badge/FastAPI-ðŸš€-turquoise?style=for-the-badge)](#)
[![LLM-Ready](https://img.shields.io/badge/LLM-Ready-purple?style=for-the-badge)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

# ðŸ” TruthLens â€” Summaries, Deep Dives & a 0â€“100 Truth Score

A founder-friendly toolkit to summarize YouTube videos and user-provided texts (e.g., book forewords), extract factual claims, gather sources, and compute a **0â€“100 Truth Score**â€”with a clean Markdown report you can share.

## ðŸ”— Table of Contents
- [Features](#features)
- [Folder Structure](#folder-structure)
- [Quickstart](#quickstart)
- [API Endpoints](#api-endpoints)
- [How Truth Scoring Works](#how-truth-scoring-works)
- [Legal & Ethics](#legal--ethics)
- [Architecture](#architecture)
- [Examples](#examples)
- [Roadmap](#roadmap)
- [License](#license)

## Features
- TL;DR & TL;DW (for videos with timestamps)
- Executive Summary (300â€“600 words) and Deep Dive
- Claim Extraction â†’ Web Corroboration â†’ **Truth Score (0â€“100)**
- â€œCritical-styleâ€ 1â€“5 â­ rating (clarity, evidence, bias)
- JSON + Markdown reports
- Optional Streamlit UI

## Folder Structure
```text
truthlens/
â”œâ”€ app/
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ config.py
â”‚  â”œâ”€ routers/
â”‚  â”‚  â”œâ”€ youtube.py
â”‚  â”‚  â”œâ”€ text.py
â”‚  â”‚  â””â”€ web.py
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ llm.py
â”‚  â”‚  â”œâ”€ transcript.py
â”‚  â”‚  â”œâ”€ summarizer.py
â”‚  â”‚  â”œâ”€ claim_extractor.py
â”‚  â”‚  â”œâ”€ searcher.py
â”‚  â”‚  â”œâ”€ fact_checker.py
â”‚  â”‚  â””â”€ scoring.py
â”‚  â”œâ”€ models/
â”‚  â”‚  â””â”€ schemas.py
â”‚  â””â”€ utils/
â”‚     â”œâ”€ logging.py
â”‚     â””â”€ rate_limit.py
â”œâ”€ ui/
â”‚  â””â”€ app.py
â”œâ”€ tests/
â”‚  â””â”€ test_scoring.py
â”œâ”€ data/
â”‚  â””â”€ .gitkeep
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ LICENSE
â””â”€ README.md
````

## Quickstart

```bash
git clone https://github.com/emcdo411/truthlens-app.git
cd truthlens-app

python -m venv .venv
# Windows:
# .venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

pip install -r requirements.txt
cp .env.example .env   # add your API keys (LLM + search + YouTube)

# Run API
make run   # FastAPI at http://localhost:8000

# Optional UI
make ui    # Streamlit at http://localhost:8501
```

## API Endpoints

* `POST /analyze/youtube`
  **Body:** `{ "url": "https://youtu.be/..." }`

* `POST /analyze/text`
  **Body:** `{ "content": "your text" }`

* `POST /analyze/web`
  **Body:** `{ "url": "https://...", "extracted_text": "..." }`
  *Note:* Provide text you have the right to use. Do **not** scrape or republish copyrighted content.

## How Truth Scoring Works

1. Extract top factual claims with an LLM.
2. Search the web (Tavily/Serper/DDG) for **independent** corroboration.
3. For each claim, assess **support vs contradiction** from snippets.
4. Aggregate to a final **0â€“100 Truth Score** with a transparent rubric and trust weighting.

## Legal & Ethics

* Use official APIs where possible (e.g., YouTube Data API).
* Avoid scraping paywalled or copyrighted text (e.g., Amazon â€œLook Insideâ€).
* Whisper transcription is **local** and **opt-in** for personal research.
* Reports include links & short quotes/snippets under fair use (brief, attributed).

## Architecture

```mermaid
flowchart LR
  A["Input"] -->|"YouTube URL / Text / Web"| ING

  subgraph ING["Ingestion"]
    direction TB
    YT["YouTube transcript or captions"]
    TX["User-provided text"]
    WB["User-extracted web text"]
  end

  ING --> N["Normalize / Clean"]
  N --> SUM["Summarizer LLM"]
  N --> CLM["Claim Extractor LLM"]

  subgraph EVID["Evidence Search"]
    direction TB
    CLM --> QUE["Query Builder"]
    QUE --> SRCH["Search Provider (Tavily, Serper, DDG)"]
    SRCH --> SNIP["Top-N results + snippets"]
  end

  SNIP --> FACT["Fact Checker LLM"]
  FACT --> ASM["Per-claim assessment (support / contradiction / rationale)"]

  subgraph OUT["Scoring & Reports"]
    direction TB
    ASM --> SCORE["Truth scoring (0-100) + trust weighting"]
    SUM --> RPT["Report builder"]
    SCORE --> RPT
    ASM --> RPT
    RPT --> MD["Markdown"]
    RPT --> JSON["JSON"]
  end

```

## Examples

* **YouTube:** Provide a URL with a public transcript â†’ get TL;DW, Summary, Deep Dive, Claims, Truth Score.
* **Books:** Paste your **own** foreword/excerpt text (or public domain text) â†’ same pipeline.

## Roadmap

* Multi-provider LLM router
* PDF ingestion
* Per-claim confidence intervals

## License

MIT

```

---





