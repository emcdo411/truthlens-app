[![Made with FastAPI](https://img.shields.io/badge/FastAPI-ðŸš€-turquoise?style=for-the-badge)](#)
[![LLM-Ready](https://img.shields.io/badge/LLM-Ready-purple?style=for-the-badge)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

# ðŸ” TruthLens â€” Summaries, Deep Dives & a 0â€“100 Truth Score

A founder-friendly toolkit to summarize YouTube videos and user-provided texts (e.g., book forewords), extract factual claims, gather sources, and compute a **0â€“100 Truth Score**â€”with a clean Markdown report you can share.

## ðŸ”— Table of Contents
- [Features](#features)
- [Folder Structure](#folder-structure)
- [Quickstart](#quickstart)
- [API Endpoints](#api-endpoints)
- [How Truth Scoring Works](#how-truth-scoring-works)
- [Legal & Ethics](#legal--ethics)
- [Architecture](#architecture)
- [Examples](#examples)
- [Roadmap](#roadmap)
- [License](#license)

## Features
- TL;DR & TL;DW (for videos with timestamps)
- Executive Summary (300â€“600 words) and Deep Dive
- Claim Extraction â†’ Web Corroboration â†’ **Truth Score (0â€“100)**
- â€œCritical-styleâ€ 1â€“5 â­ rating (clarity, evidence, bias)
- JSON + Markdown reports
- Optional Streamlit UI

## Folder Structure
```text
truthlens/
â”œâ”€ app/
â”‚  â”œâ”€ main.py
â”‚  â”œâ”€ config.py
â”‚  â”œâ”€ routers/
â”‚  â”‚  â”œâ”€ youtube.py
â”‚  â”‚  â”œâ”€ text.py
â”‚  â”‚  â””â”€ web.py
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ llm.py
â”‚  â”‚  â”œâ”€ transcript.py
â”‚  â”‚  â”œâ”€ summarizer.py
â”‚  â”‚  â”œâ”€ claim_extractor.py
â”‚  â”‚  â”œâ”€ searcher.py
â”‚  â”‚  â”œâ”€ fact_checker.py
â”‚  â”‚  â””â”€ scoring.py
â”‚  â”œâ”€ models/
â”‚  â”‚  â””â”€ schemas.py
â”‚  â””â”€ utils/
â”‚     â”œâ”€ logging.py
â”‚     â””â”€ rate_limit.py
â”œâ”€ ui/
â”‚  â””â”€ app.py
â”œâ”€ tests/
â”‚  â””â”€ test_scoring.py
â”œâ”€ data/
â”‚  â””â”€ .gitkeep
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ LICENSE
â””â”€ README.md
````

## Quickstart

```bash
git clone https://github.com/emcdo411/truthlens-app.git
cd truthlens-app

python -m venv .venv
# Windows:
# .venv\Scripts\activate
# macOS/Linux:
# source .venv/bin/activate

pip install -r requirements.txt
cp .env.example .env   # add your API keys (LLM + search + YouTube)

# Run API
make run   # FastAPI at http://localhost:8000

# Optional UI
make ui    # Streamlit at http://localhost:8501
```

## API Endpoints

* `POST /analyze/youtube`
  **Body:** `{ "url": "https://youtu.be/..." }`

* `POST /analyze/text`
  **Body:** `{ "content": "your text" }`

* `POST /analyze/web`
  **Body:** `{ "url": "https://...", "extracted_text": "..." }`
  *Note:* Provide text you have the right to use. Do **not** scrape or republish copyrighted content.

## How Truth Scoring Works

1. Extract top factual claims with an LLM.
2. Search the web (Tavily/Serper/DDG) for **independent** corroboration.
3. For each claim, assess **support vs contradiction** from snippets.
4. Aggregate to a final **0â€“100 Truth Score** with a transparent rubric and trust weighting.

## Legal & Ethics

* Use official APIs where possible (e.g., YouTube Data API).
* Avoid scraping paywalled or copyrighted text (e.g., Amazon â€œLook Insideâ€).
* Whisper transcription is **local** and **opt-in** for personal research.
* Reports include links & short quotes/snippets under fair use (brief, attributed).

## Architecture

```mermaid
flowchart LR
  %% High-level pipeline with subgraphs (GitHub supports Mermaid)
  A[Input] -->|YouTube URL / Text / Web| B[Ingestion]

  subgraph Ingestion
    B1[YouTube transcript\n(API or public captions)]
    B2[User-provided text]
    B3[Web page text\n(user-extracted)]
  end

  B --> C[Normalization]
  C --> D[LLM Summarizer]
  C --> E[Claim Extractor]

  subgraph Evidence Search
    direction TB
    E --> S1[Query Builder]
    S1 --> S2[Search Provider\n(Tavily/Serper/DDG)]
    S2 --> S3[Top N Results + Snippets]
  end

  S3 --> F[Fact Checker LLM]
  F --> G[Per-Claim Assessment\nsupport/contradiction/rationale]

  subgraph Scoring & Reports
    direction TB
    G --> H[Truth Scoring (0â€“100)\n+ Trust weighting]
    D --> I[Executive Summary\n+ Deep Dive]
    H --> J[Markdown Report]
    I --> J
    G --> J
    J --> K[JSON Output]
  end
```

## Examples

* **YouTube:** Provide a URL with a public transcript â†’ get TL;DW, Summary, Deep Dive, Claims, Truth Score.
* **Books:** Paste your **own** foreword/excerpt text (or public domain text) â†’ same pipeline.

## Roadmap

* Multi-provider LLM router
* PDF ingestion
* Per-claim confidence intervals

## License

MIT

```

---


